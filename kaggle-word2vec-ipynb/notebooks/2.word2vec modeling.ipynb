{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "# nltk.download()\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join('..', 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入无标签数据\n",
    "用于训练生成word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('unlabeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和第一个ipython notebook一样做数据的预处理\n",
    "稍稍有一点不一样的是，我们留了个候选，可以去除停用词，也可以不去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eng_stopwords = set(stopwords.words('english'))\n",
    "eng_stopwords = {}.fromkeys([ line.rstrip() for line in open('../stopwords.txt')])\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') #加载英文的划分句子的模型\n",
    "\n",
    "def print_call_counts(f):\n",
    "    n = 0\n",
    "    def wrapped(*args, **kwargs):\n",
    "        nonlocal n\n",
    "        n += 1\n",
    "        if n % 1000 == 1:\n",
    "            print('method {} called {} times'.format(f.__name__, n))\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@print_call_counts\n",
    "def split_sentences(review):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1001 times\n",
      "method split_sentences called 2001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 3001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 4001 times\n",
      "method split_sentences called 5001 times\n",
      "method split_sentences called 6001 times\n",
      "method split_sentences called 7001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 8001 times\n",
      "method split_sentences called 9001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 10001 times\n",
      "method split_sentences called 11001 times\n",
      "method split_sentences called 12001 times\n",
      "method split_sentences called 13001 times\n",
      "method split_sentences called 14001 times\n",
      "method split_sentences called 15001 times\n",
      "method split_sentences called 16001 times\n",
      "method split_sentences called 17001 times\n",
      "method split_sentences called 18001 times\n",
      "method split_sentences called 19001 times\n",
      "method split_sentences called 20001 times\n",
      "method split_sentences called 21001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 22001 times\n",
      "method split_sentences called 23001 times\n",
      "method split_sentences called 24001 times\n",
      "method split_sentences called 25001 times\n",
      "method split_sentences called 26001 times\n",
      "method split_sentences called 27001 times\n",
      "method split_sentences called 28001 times\n",
      "method split_sentences called 29001 times\n",
      "method split_sentences called 30001 times\n",
      "method split_sentences called 31001 times\n",
      "method split_sentences called 32001 times\n",
      "method split_sentences called 33001 times\n",
      "method split_sentences called 34001 times\n",
      "method split_sentences called 35001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 36001 times\n",
      "method split_sentences called 37001 times\n",
      "method split_sentences called 38001 times\n",
      "method split_sentences called 39001 times\n",
      "method split_sentences called 40001 times\n",
      "method split_sentences called 41001 times\n",
      "method split_sentences called 42001 times\n",
      "method split_sentences called 43001 times\n",
      "method split_sentences called 44001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 45001 times\n",
      "method split_sentences called 46001 times\n",
      "method split_sentences called 47001 times\n",
      "method split_sentences called 48001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 49001 times\n",
      "Wall time: 31min 45s\n",
      "50000 reviews -> 537851 sentences\n"
     ]
    }
   ],
   "source": [
    "%time sentences = sum(df.review.apply(split_sentences), [])\n",
    "print('{} reviews -> {} sentences'.format(len(df), len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用gensim训练词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name =  300features_40minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "# 设定词向量训练的参数\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)\n",
    "print(\"model_name = \", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-16 22:32:25,699 : INFO : collecting all words and their counts\n",
      "2019-03-16 22:32:25,701 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-16 22:32:25,939 : INFO : PROGRESS: at sentence #10000, processed 225072 words, keeping 17237 word types\n",
      "2019-03-16 22:32:26,130 : INFO : PROGRESS: at sentence #20000, processed 443536 words, keeping 24570 word types\n",
      "2019-03-16 22:32:26,287 : INFO : PROGRESS: at sentence #30000, processed 666343 words, keeping 29785 word types\n",
      "2019-03-16 22:32:26,430 : INFO : PROGRESS: at sentence #40000, processed 886903 words, keeping 33939 word types\n",
      "2019-03-16 22:32:26,558 : INFO : PROGRESS: at sentence #50000, processed 1103863 words, keeping 37503 word types\n",
      "2019-03-16 22:32:26,687 : INFO : PROGRESS: at sentence #60000, processed 1327231 words, keeping 40738 word types\n",
      "2019-03-16 22:32:26,840 : INFO : PROGRESS: at sentence #70000, processed 1550828 words, keeping 43603 word types\n",
      "2019-03-16 22:32:26,983 : INFO : PROGRESS: at sentence #80000, processed 1772824 words, keeping 46155 word types\n",
      "2019-03-16 22:32:27,136 : INFO : PROGRESS: at sentence #90000, processed 1987492 words, keeping 48328 word types\n",
      "2019-03-16 22:32:27,295 : INFO : PROGRESS: at sentence #100000, processed 2210772 words, keeping 50551 word types\n",
      "2019-03-16 22:32:27,450 : INFO : PROGRESS: at sentence #110000, processed 2435496 words, keeping 52762 word types\n",
      "2019-03-16 22:32:27,608 : INFO : PROGRESS: at sentence #120000, processed 2658449 words, keeping 54893 word types\n",
      "2019-03-16 22:32:27,762 : INFO : PROGRESS: at sentence #130000, processed 2877962 words, keeping 56598 word types\n",
      "2019-03-16 22:32:27,916 : INFO : PROGRESS: at sentence #140000, processed 3098235 words, keeping 58352 word types\n",
      "2019-03-16 22:32:28,070 : INFO : PROGRESS: at sentence #150000, processed 3315370 words, keeping 60013 word types\n",
      "2019-03-16 22:32:28,221 : INFO : PROGRESS: at sentence #160000, processed 3536039 words, keeping 61691 word types\n",
      "2019-03-16 22:32:28,374 : INFO : PROGRESS: at sentence #170000, processed 3758385 words, keeping 63292 word types\n",
      "2019-03-16 22:32:28,538 : INFO : PROGRESS: at sentence #180000, processed 3979413 words, keeping 64846 word types\n",
      "2019-03-16 22:32:28,763 : INFO : PROGRESS: at sentence #190000, processed 4203546 words, keeping 66403 word types\n",
      "2019-03-16 22:32:28,973 : INFO : PROGRESS: at sentence #200000, processed 4429481 words, keeping 67924 word types\n",
      "2019-03-16 22:32:29,123 : INFO : PROGRESS: at sentence #210000, processed 4652920 words, keeping 69248 word types\n",
      "2019-03-16 22:32:29,286 : INFO : PROGRESS: at sentence #220000, processed 4870835 words, keeping 70567 word types\n",
      "2019-03-16 22:32:29,464 : INFO : PROGRESS: at sentence #230000, processed 5093104 words, keeping 71912 word types\n",
      "2019-03-16 22:32:29,677 : INFO : PROGRESS: at sentence #240000, processed 5311435 words, keeping 73234 word types\n",
      "2019-03-16 22:32:29,924 : INFO : PROGRESS: at sentence #250000, processed 5532195 words, keeping 74486 word types\n",
      "2019-03-16 22:32:30,288 : INFO : PROGRESS: at sentence #260000, processed 5751629 words, keeping 75693 word types\n",
      "2019-03-16 22:32:30,438 : INFO : PROGRESS: at sentence #270000, processed 5973493 words, keeping 76829 word types\n",
      "2019-03-16 22:32:30,593 : INFO : PROGRESS: at sentence #280000, processed 6191000 words, keeping 77953 word types\n",
      "2019-03-16 22:32:30,722 : INFO : PROGRESS: at sentence #290000, processed 6415980 words, keeping 79135 word types\n",
      "2019-03-16 22:32:30,875 : INFO : PROGRESS: at sentence #300000, processed 6634726 words, keeping 80229 word types\n",
      "2019-03-16 22:32:31,009 : INFO : PROGRESS: at sentence #310000, processed 6857605 words, keeping 81308 word types\n",
      "2019-03-16 22:32:31,161 : INFO : PROGRESS: at sentence #320000, processed 7077123 words, keeping 82421 word types\n",
      "2019-03-16 22:32:31,318 : INFO : PROGRESS: at sentence #330000, processed 7298667 words, keeping 83509 word types\n",
      "2019-03-16 22:32:31,490 : INFO : PROGRESS: at sentence #340000, processed 7516302 words, keeping 84445 word types\n",
      "2019-03-16 22:32:31,667 : INFO : PROGRESS: at sentence #350000, processed 7735101 words, keeping 85566 word types\n",
      "2019-03-16 22:32:31,798 : INFO : PROGRESS: at sentence #360000, processed 7956254 words, keeping 86544 word types\n",
      "2019-03-16 22:32:31,943 : INFO : PROGRESS: at sentence #370000, processed 8177574 words, keeping 87489 word types\n",
      "2019-03-16 22:32:32,164 : INFO : PROGRESS: at sentence #380000, processed 8395550 words, keeping 88515 word types\n",
      "2019-03-16 22:32:32,402 : INFO : PROGRESS: at sentence #390000, processed 8616518 words, keeping 89500 word types\n",
      "2019-03-16 22:32:32,571 : INFO : PROGRESS: at sentence #400000, processed 8835616 words, keeping 90470 word types\n",
      "2019-03-16 22:32:32,765 : INFO : PROGRESS: at sentence #410000, processed 9055384 words, keeping 91344 word types\n",
      "2019-03-16 22:32:33,176 : INFO : PROGRESS: at sentence #420000, processed 9276296 words, keeping 92245 word types\n",
      "2019-03-16 22:32:33,520 : INFO : PROGRESS: at sentence #430000, processed 9494459 words, keeping 93176 word types\n",
      "2019-03-16 22:32:33,728 : INFO : PROGRESS: at sentence #440000, processed 9719312 words, keeping 94119 word types\n",
      "2019-03-16 22:32:33,942 : INFO : PROGRESS: at sentence #450000, processed 9936915 words, keeping 94980 word types\n",
      "2019-03-16 22:32:34,486 : INFO : PROGRESS: at sentence #460000, processed 10160053 words, keeping 95781 word types\n",
      "2019-03-16 22:32:34,709 : INFO : PROGRESS: at sentence #470000, processed 10380740 words, keeping 96637 word types\n",
      "2019-03-16 22:32:34,937 : INFO : PROGRESS: at sentence #480000, processed 10599172 words, keeping 97471 word types\n",
      "2019-03-16 22:32:35,141 : INFO : PROGRESS: at sentence #490000, processed 10816559 words, keeping 98279 word types\n",
      "2019-03-16 22:32:35,342 : INFO : PROGRESS: at sentence #500000, processed 11032175 words, keeping 99064 word types\n",
      "2019-03-16 22:32:35,608 : INFO : PROGRESS: at sentence #510000, processed 11254508 words, keeping 99930 word types\n",
      "2019-03-16 22:32:35,803 : INFO : PROGRESS: at sentence #520000, processed 11481357 words, keeping 100836 word types\n",
      "2019-03-16 22:32:36,265 : INFO : PROGRESS: at sentence #530000, processed 11704018 words, keeping 101618 word types\n",
      "2019-03-16 22:32:36,410 : INFO : collected 102304 word types from a corpus of 11877522 raw words and 537851 sentences\n",
      "2019-03-16 22:32:36,412 : INFO : Loading a fresh vocabulary\n",
      "2019-03-16 22:32:36,612 : INFO : effective_min_count=40 retains 13056 unique words (12% of original 102304, drops 89248)\n",
      "2019-03-16 22:32:36,614 : INFO : effective_min_count=40 leaves 11401019 word corpus (95% of original 11877522, drops 476503)\n",
      "2019-03-16 22:32:36,771 : INFO : deleting the raw counts dictionary of 102304 items\n",
      "2019-03-16 22:32:36,784 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-03-16 22:32:36,790 : INFO : downsampling leaves estimated 8394665 word corpus (73.6% of prior 11401019)\n",
      "2019-03-16 22:32:36,973 : INFO : estimated required memory for 13056 words and 300 dimensions: 37862400 bytes\n",
      "2019-03-16 22:32:36,975 : INFO : resetting layer weights\n",
      "2019-03-16 22:32:37,613 : INFO : training model with 4 workers on 13056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-16 22:32:38,688 : INFO : EPOCH 1 - PROGRESS: at 2.83% examples, 231622 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:39,698 : INFO : EPOCH 1 - PROGRESS: at 5.69% examples, 234858 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:32:40,736 : INFO : EPOCH 1 - PROGRESS: at 9.08% examples, 247279 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:41,785 : INFO : EPOCH 1 - PROGRESS: at 11.91% examples, 242520 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:42,800 : INFO : EPOCH 1 - PROGRESS: at 14.32% examples, 234371 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:43,802 : INFO : EPOCH 1 - PROGRESS: at 17.22% examples, 235245 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:44,806 : INFO : EPOCH 1 - PROGRESS: at 20.28% examples, 238621 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:45,808 : INFO : EPOCH 1 - PROGRESS: at 23.96% examples, 247411 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:46,846 : INFO : EPOCH 1 - PROGRESS: at 27.28% examples, 249389 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:32:47,865 : INFO : EPOCH 1 - PROGRESS: at 30.16% examples, 247946 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:32:48,921 : INFO : EPOCH 1 - PROGRESS: at 32.81% examples, 244696 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-16 22:32:49,942 : INFO : EPOCH 1 - PROGRESS: at 35.48% examples, 242693 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:32:50,955 : INFO : EPOCH 1 - PROGRESS: at 38.28% examples, 242239 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:32:51,960 : INFO : EPOCH 1 - PROGRESS: at 41.55% examples, 244416 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:52,979 : INFO : EPOCH 1 - PROGRESS: at 44.11% examples, 241986 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:32:54,000 : INFO : EPOCH 1 - PROGRESS: at 48.07% examples, 247145 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:32:55,013 : INFO : EPOCH 1 - PROGRESS: at 50.74% examples, 245744 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:56,046 : INFO : EPOCH 1 - PROGRESS: at 53.69% examples, 245384 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:32:57,056 : INFO : EPOCH 1 - PROGRESS: at 56.73% examples, 245695 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:32:58,070 : INFO : EPOCH 1 - PROGRESS: at 59.32% examples, 244199 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:32:59,098 : INFO : EPOCH 1 - PROGRESS: at 62.02% examples, 243010 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:00,151 : INFO : EPOCH 1 - PROGRESS: at 64.57% examples, 241041 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:01,168 : INFO : EPOCH 1 - PROGRESS: at 67.76% examples, 242039 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:33:02,182 : INFO : EPOCH 1 - PROGRESS: at 70.98% examples, 242939 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:03,209 : INFO : EPOCH 1 - PROGRESS: at 74.44% examples, 244503 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:04,229 : INFO : EPOCH 1 - PROGRESS: at 76.30% examples, 240956 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:05,244 : INFO : EPOCH 1 - PROGRESS: at 78.74% examples, 239512 words/s, in_qsize 7, out_qsize 2\n",
      "2019-03-16 22:33:06,271 : INFO : EPOCH 1 - PROGRESS: at 80.60% examples, 236329 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:07,286 : INFO : EPOCH 1 - PROGRESS: at 83.87% examples, 237512 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:08,326 : INFO : EPOCH 1 - PROGRESS: at 87.04% examples, 238191 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:09,369 : INFO : EPOCH 1 - PROGRESS: at 90.08% examples, 238363 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:10,392 : INFO : EPOCH 1 - PROGRESS: at 93.70% examples, 239970 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:11,409 : INFO : EPOCH 1 - PROGRESS: at 98.42% examples, 244670 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:11,926 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-16 22:33:11,965 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-16 22:33:11,984 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-16 22:33:12,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-16 22:33:12,007 : INFO : EPOCH - 1 : training on 11877522 raw words (8392750 effective words) took 34.4s, 244307 effective words/s\n",
      "2019-03-16 22:33:13,047 : INFO : EPOCH 2 - PROGRESS: at 3.60% examples, 303007 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:14,126 : INFO : EPOCH 2 - PROGRESS: at 6.53% examples, 264792 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:15,147 : INFO : EPOCH 2 - PROGRESS: at 9.47% examples, 257109 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:33:16,161 : INFO : EPOCH 2 - PROGRESS: at 12.81% examples, 262213 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:17,165 : INFO : EPOCH 2 - PROGRESS: at 16.72% examples, 274330 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:18,172 : INFO : EPOCH 2 - PROGRESS: at 20.20% examples, 277490 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:19,189 : INFO : EPOCH 2 - PROGRESS: at 24.72% examples, 291335 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:20,201 : INFO : EPOCH 2 - PROGRESS: at 28.72% examples, 295782 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:21,202 : INFO : EPOCH 2 - PROGRESS: at 32.32% examples, 296562 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:22,206 : INFO : EPOCH 2 - PROGRESS: at 34.82% examples, 288075 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:23,250 : INFO : EPOCH 2 - PROGRESS: at 37.80% examples, 283896 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:24,259 : INFO : EPOCH 2 - PROGRESS: at 41.99% examples, 289315 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:33:25,262 : INFO : EPOCH 2 - PROGRESS: at 44.79% examples, 284977 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:26,280 : INFO : EPOCH 2 - PROGRESS: at 48.73% examples, 287886 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:27,286 : INFO : EPOCH 2 - PROGRESS: at 52.64% examples, 290168 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:28,289 : INFO : EPOCH 2 - PROGRESS: at 55.47% examples, 287027 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:29,297 : INFO : EPOCH 2 - PROGRESS: at 58.56% examples, 285405 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:30,302 : INFO : EPOCH 2 - PROGRESS: at 61.77% examples, 284388 words/s, in_qsize 6, out_qsize 2\n",
      "2019-03-16 22:33:31,308 : INFO : EPOCH 2 - PROGRESS: at 65.66% examples, 286370 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:32,394 : INFO : EPOCH 2 - PROGRESS: at 70.06% examples, 289118 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:33,399 : INFO : EPOCH 2 - PROGRESS: at 72.15% examples, 283769 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:34,419 : INFO : EPOCH 2 - PROGRESS: at 74.61% examples, 279981 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:35,439 : INFO : EPOCH 2 - PROGRESS: at 78.24% examples, 280751 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:36,446 : INFO : EPOCH 2 - PROGRESS: at 81.19% examples, 279294 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:33:37,455 : INFO : EPOCH 2 - PROGRESS: at 85.35% examples, 282082 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:33:38,462 : INFO : EPOCH 2 - PROGRESS: at 89.24% examples, 283611 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:39,512 : INFO : EPOCH 2 - PROGRESS: at 92.32% examples, 282002 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:40,519 : INFO : EPOCH 2 - PROGRESS: at 95.93% examples, 282690 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:41,537 : INFO : EPOCH 2 - PROGRESS: at 99.16% examples, 282281 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:41,739 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-16 22:33:41,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-16 22:33:41,767 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-16 22:33:41,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-16 22:33:41,824 : INFO : EPOCH - 2 : training on 11877522 raw words (8394116 effective words) took 29.8s, 281897 effective words/s\n",
      "2019-03-16 22:33:42,905 : INFO : EPOCH 3 - PROGRESS: at 3.19% examples, 264560 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:43,915 : INFO : EPOCH 3 - PROGRESS: at 5.43% examples, 226742 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:33:44,934 : INFO : EPOCH 3 - PROGRESS: at 8.30% examples, 229646 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:33:45,972 : INFO : EPOCH 3 - PROGRESS: at 10.97% examples, 226509 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:46,979 : INFO : EPOCH 3 - PROGRESS: at 14.66% examples, 242662 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:48,007 : INFO : EPOCH 3 - PROGRESS: at 17.39% examples, 238783 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:49,082 : INFO : EPOCH 3 - PROGRESS: at 20.53% examples, 240330 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:50,082 : INFO : EPOCH 3 - PROGRESS: at 23.28% examples, 239415 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:51,094 : INFO : EPOCH 3 - PROGRESS: at 26.42% examples, 241455 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:33:52,202 : INFO : EPOCH 3 - PROGRESS: at 28.99% examples, 236048 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:53,218 : INFO : EPOCH 3 - PROGRESS: at 32.05% examples, 237922 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:54,276 : INFO : EPOCH 3 - PROGRESS: at 34.98% examples, 237507 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:55,310 : INFO : EPOCH 3 - PROGRESS: at 38.28% examples, 240225 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-16 22:33:56,320 : INFO : EPOCH 3 - PROGRESS: at 41.48% examples, 241957 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:33:57,380 : INFO : EPOCH 3 - PROGRESS: at 44.70% examples, 242728 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:33:58,409 : INFO : EPOCH 3 - PROGRESS: at 47.30% examples, 240854 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:33:59,443 : INFO : EPOCH 3 - PROGRESS: at 50.57% examples, 242318 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:00,452 : INFO : EPOCH 3 - PROGRESS: at 52.97% examples, 239801 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:34:01,510 : INFO : EPOCH 3 - PROGRESS: at 55.97% examples, 239828 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:02,550 : INFO : EPOCH 3 - PROGRESS: at 59.49% examples, 242108 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:03,567 : INFO : EPOCH 3 - PROGRESS: at 62.11% examples, 240826 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:04,589 : INFO : EPOCH 3 - PROGRESS: at 64.66% examples, 239300 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:05,605 : INFO : EPOCH 3 - PROGRESS: at 67.00% examples, 237374 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:06,648 : INFO : EPOCH 3 - PROGRESS: at 69.88% examples, 237043 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:07,656 : INFO : EPOCH 3 - PROGRESS: at 73.94% examples, 240921 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:08,707 : INFO : EPOCH 3 - PROGRESS: at 77.62% examples, 243064 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:09,710 : INFO : EPOCH 3 - PROGRESS: at 80.69% examples, 243442 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:34:10,754 : INFO : EPOCH 3 - PROGRESS: at 83.53% examples, 242936 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:11,785 : INFO : EPOCH 3 - PROGRESS: at 86.38% examples, 242570 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:12,787 : INFO : EPOCH 3 - PROGRESS: at 88.91% examples, 241557 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:13,803 : INFO : EPOCH 3 - PROGRESS: at 90.86% examples, 238946 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:14,806 : INFO : EPOCH 3 - PROGRESS: at 93.03% examples, 237026 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:15,852 : INFO : EPOCH 3 - PROGRESS: at 95.59% examples, 236167 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:34:16,875 : INFO : EPOCH 3 - PROGRESS: at 97.98% examples, 235105 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:17,634 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-16 22:34:17,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-16 22:34:17,709 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-16 22:34:17,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-16 22:34:17,721 : INFO : EPOCH - 3 : training on 11877522 raw words (8393502 effective words) took 35.8s, 234265 effective words/s\n",
      "2019-03-16 22:34:18,772 : INFO : EPOCH 4 - PROGRESS: at 3.34% examples, 278353 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:34:19,784 : INFO : EPOCH 4 - PROGRESS: at 6.02% examples, 250904 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:34:20,819 : INFO : EPOCH 4 - PROGRESS: at 9.33% examples, 255943 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:21,905 : INFO : EPOCH 4 - PROGRESS: at 12.08% examples, 245044 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:34:22,908 : INFO : EPOCH 4 - PROGRESS: at 15.25% examples, 249388 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:23,921 : INFO : EPOCH 4 - PROGRESS: at 17.97% examples, 244998 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:24,952 : INFO : EPOCH 4 - PROGRESS: at 20.72% examples, 242192 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:26,015 : INFO : EPOCH 4 - PROGRESS: at 23.45% examples, 239217 words/s, in_qsize 5, out_qsize 2\n",
      "2019-03-16 22:34:27,021 : INFO : EPOCH 4 - PROGRESS: at 26.51% examples, 240647 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:28,041 : INFO : EPOCH 4 - PROGRESS: at 30.49% examples, 249002 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:34:29,092 : INFO : EPOCH 4 - PROGRESS: at 33.41% examples, 247708 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:30,104 : INFO : EPOCH 4 - PROGRESS: at 36.96% examples, 251915 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:31,111 : INFO : EPOCH 4 - PROGRESS: at 40.64% examples, 256170 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:32,137 : INFO : EPOCH 4 - PROGRESS: at 43.23% examples, 253080 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:33,155 : INFO : EPOCH 4 - PROGRESS: at 46.38% examples, 253299 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:34,160 : INFO : EPOCH 4 - PROGRESS: at 49.30% examples, 252823 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:35,164 : INFO : EPOCH 4 - PROGRESS: at 52.29% examples, 252432 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:36,166 : INFO : EPOCH 4 - PROGRESS: at 55.05% examples, 251332 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:37,239 : INFO : EPOCH 4 - PROGRESS: at 58.04% examples, 250550 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:38,269 : INFO : EPOCH 4 - PROGRESS: at 60.92% examples, 249657 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:39,286 : INFO : EPOCH 4 - PROGRESS: at 63.56% examples, 248047 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:40,299 : INFO : EPOCH 4 - PROGRESS: at 66.15% examples, 246588 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:34:41,349 : INFO : EPOCH 4 - PROGRESS: at 69.37% examples, 246978 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:42,415 : INFO : EPOCH 4 - PROGRESS: at 72.24% examples, 246008 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:34:43,457 : INFO : EPOCH 4 - PROGRESS: at 75.21% examples, 245666 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:44,478 : INFO : EPOCH 4 - PROGRESS: at 77.98% examples, 244994 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:45,501 : INFO : EPOCH 4 - PROGRESS: at 80.59% examples, 243827 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:46,512 : INFO : EPOCH 4 - PROGRESS: at 83.71% examples, 244330 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:47,569 : INFO : EPOCH 4 - PROGRESS: at 87.30% examples, 245835 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:48,581 : INFO : EPOCH 4 - PROGRESS: at 90.09% examples, 245303 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:49,597 : INFO : EPOCH 4 - PROGRESS: at 92.58% examples, 243913 words/s, in_qsize 6, out_qsize 2\n",
      "2019-03-16 22:34:50,628 : INFO : EPOCH 4 - PROGRESS: at 95.44% examples, 243553 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:51,660 : INFO : EPOCH 4 - PROGRESS: at 98.50% examples, 243839 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:52,146 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-16 22:34:52,156 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-16 22:34:52,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-16 22:34:52,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-16 22:34:52,276 : INFO : EPOCH - 4 : training on 11877522 raw words (8392998 effective words) took 34.5s, 243143 effective words/s\n",
      "2019-03-16 22:34:53,351 : INFO : EPOCH 5 - PROGRESS: at 2.66% examples, 220065 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:34:54,363 : INFO : EPOCH 5 - PROGRESS: at 5.60% examples, 232109 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:34:55,381 : INFO : EPOCH 5 - PROGRESS: at 8.57% examples, 235692 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:56,441 : INFO : EPOCH 5 - PROGRESS: at 11.91% examples, 243412 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:34:57,457 : INFO : EPOCH 5 - PROGRESS: at 14.66% examples, 240615 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:34:58,463 : INFO : EPOCH 5 - PROGRESS: at 17.56% examples, 240315 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:34:59,479 : INFO : EPOCH 5 - PROGRESS: at 20.63% examples, 242636 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:35:00,492 : INFO : EPOCH 5 - PROGRESS: at 23.54% examples, 242775 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-16 22:35:01,530 : INFO : EPOCH 5 - PROGRESS: at 26.60% examples, 243039 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:02,540 : INFO : EPOCH 5 - PROGRESS: at 29.89% examples, 245940 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-16 22:35:03,560 : INFO : EPOCH 5 - PROGRESS: at 33.33% examples, 249316 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:35:04,581 : INFO : EPOCH 5 - PROGRESS: at 36.20% examples, 248662 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:05,605 : INFO : EPOCH 5 - PROGRESS: at 39.45% examples, 250237 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:35:06,647 : INFO : EPOCH 5 - PROGRESS: at 41.99% examples, 246756 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:07,654 : INFO : EPOCH 5 - PROGRESS: at 44.61% examples, 244813 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:08,667 : INFO : EPOCH 5 - PROGRESS: at 47.31% examples, 243467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:09,670 : INFO : EPOCH 5 - PROGRESS: at 49.92% examples, 242015 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:10,682 : INFO : EPOCH 5 - PROGRESS: at 51.86% examples, 237512 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:35:11,726 : INFO : EPOCH 5 - PROGRESS: at 54.54% examples, 236358 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:35:12,728 : INFO : EPOCH 5 - PROGRESS: at 57.46% examples, 236888 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:35:13,758 : INFO : EPOCH 5 - PROGRESS: at 60.51% examples, 237362 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:14,770 : INFO : EPOCH 5 - PROGRESS: at 63.65% examples, 238291 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:35:15,776 : INFO : EPOCH 5 - PROGRESS: at 66.15% examples, 237085 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:16,785 : INFO : EPOCH 5 - PROGRESS: at 68.36% examples, 234828 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:17,824 : INFO : EPOCH 5 - PROGRESS: at 69.81% examples, 229951 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:18,844 : INFO : EPOCH 5 - PROGRESS: at 71.91% examples, 227748 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:19,893 : INFO : EPOCH 5 - PROGRESS: at 74.79% examples, 227803 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-16 22:35:20,913 : INFO : EPOCH 5 - PROGRESS: at 77.54% examples, 227821 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:21,925 : INFO : EPOCH 5 - PROGRESS: at 80.42% examples, 228134 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:35:22,964 : INFO : EPOCH 5 - PROGRESS: at 83.01% examples, 227532 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:24,001 : INFO : EPOCH 5 - PROGRESS: at 86.04% examples, 228094 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-16 22:35:25,003 : INFO : EPOCH 5 - PROGRESS: at 89.33% examples, 229522 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:26,025 : INFO : EPOCH 5 - PROGRESS: at 93.20% examples, 231970 words/s, in_qsize 7, out_qsize 1\n",
      "2019-03-16 22:35:27,068 : INFO : EPOCH 5 - PROGRESS: at 97.08% examples, 234553 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-16 22:35:27,713 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-16 22:35:27,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-16 22:35:27,762 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-16 22:35:27,784 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-16 22:35:27,786 : INFO : EPOCH - 5 : training on 11877522 raw words (8394974 effective words) took 35.5s, 236746 effective words/s\n",
      "2019-03-16 22:35:27,795 : INFO : training on a 59387610 raw words (41968340 effective words) took 170.2s, 246611 effective words/s\n",
      "2019-03-16 22:35:27,797 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-03-16 22:35:27,838 : INFO : saving Word2Vec object under ..\\models\\300features_40minwords_10context.model, separately None\n",
      "2019-03-16 22:35:27,842 : INFO : not storing attribute vectors_norm\n",
      "2019-03-16 22:35:27,845 : INFO : not storing attribute cum_table\n",
      "2019-03-16 22:35:29,069 : INFO : saved ..\\models\\300features_40minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('..', 'models', model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看训练的词向量结果如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(model.doesnt_match('france england germany berlin'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6647160053253174),\n",
       " ('lady', 0.6140276193618774),\n",
       " ('lad', 0.5904245376586914),\n",
       " ('guy', 0.5348434448242188),\n",
       " ('person', 0.5302190780639648),\n",
       " ('chap', 0.5207657814025879),\n",
       " ('men', 0.5132219791412354),\n",
       " ('priest', 0.5081614255905151),\n",
       " ('soldier', 0.5078833103179932),\n",
       " ('monk', 0.5074741840362549)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6432243585586548),\n",
       " ('maid', 0.6326397657394409),\n",
       " ('temple', 0.6207026243209839),\n",
       " ('stripper', 0.6190794706344604),\n",
       " ('belle', 0.6153777837753296),\n",
       " ('eva', 0.614830493927002),\n",
       " ('bride', 0.6097807884216309),\n",
       " ('housekeeper', 0.6069656014442444),\n",
       " ('rose', 0.6029557585716248),\n",
       " ('katherine', 0.599675178527832)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyPrograms\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7801300287246704),\n",
       " ('atrocious', 0.7467272281646729),\n",
       " ('horrible', 0.7357441186904907),\n",
       " ('abysmal', 0.7168046832084656),\n",
       " ('dreadful', 0.7146291732788086),\n",
       " ('horrid', 0.6934999227523804),\n",
       " ('lousy', 0.6796210408210754),\n",
       " ('horrendous', 0.678307294845581),\n",
       " ('appalling', 0.6761072278022766),\n",
       " ('laughable', 0.6340218782424927)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
